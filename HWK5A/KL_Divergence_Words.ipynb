{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jenishkothari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jenishkothari/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from scipy.stats import entropy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(duc_path):\n",
    "    docs_path = os.path.join(duc_path, \"Docs\")\n",
    "    summaries_path = os.path.join(duc_path, \"Summaries\")\n",
    "    \n",
    "    documents = {}\n",
    "    gold_summaries = {}\n",
    "\n",
    "    for filename in os.listdir(docs_path):\n",
    "        filepath = os.path.join(docs_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                documents[filename] = f.read()\n",
    "\n",
    "    for filename in os.listdir(summaries_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(summaries_path, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                match = re.search(r'Abstract:(.*?)(?:Introduction:|$)', content, re.DOTALL)\n",
    "                if match:\n",
    "                    doc_key = filename.replace(\".txt\", \"\")\n",
    "                    gold_summaries[doc_key] = match.group(1).strip()\n",
    "    \n",
    "    return documents, gold_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Document ID: FBIS-41815\n",
      "<DOC>\n",
      "<DOCNO> FBIS3-41815 </DOCNO>\n",
      "<HT>    \"jptep001__l94053\" </HT>\n",
      "\n",
      "\n",
      "<HEADER>\n",
      "<AU>   JPRS-TEP-94-001-L </AU>\n",
      "Document Type:JPRS \n",
      "Document Title:Epidemiology \n",
      "\n",
      "<DATE1>  25 February 1994 </DATE1>\n",
      "\n",
      "</HEADER>\n",
      "\n",
      "<F P=100> WEST EUROPE </F>\n",
      "<F P=101> UNITED KINGDOM </F>\n",
      "<H3> <TI>   Concern Over Transmissio...\n",
      "\n",
      "🟨 Matching Gold Summary:\n",
      "\n",
      "\n",
      "📄 Document ID: AP881017-0235\n",
      "\n",
      "<DOC>\n",
      "<DOCNO> AP881017-0235 </DOCNO>\n",
      "<FILEID>AP-NR-10-17-88 2351EDT</FILEID>\n",
      "<FIRST>a i BC-ChannelTunnel ADV30   10-17 0995</FIRST>\n",
      "<SECOND>BC-Channel Tunnel, ADV 30,1025</SECOND>\n",
      "<NOTE>$Adv30</NOTE>\n",
      "<NOTE>For Release Sunday, Oct. 30, or Thereafter</NOTE>\n",
      "<HEAD>Gigantic Tunnel Project Inches Toward...\n",
      "\n",
      "🟨 Matching Gold Summary:\n",
      "Construction of a 31-mile tunnel, 24 miles of it underwater, is underway between Dover, England to the coast of France. Scheduled to be completed in 1993, it will cut the London-Paris journey from six hours to three. It will enable freight to travel on a single train instead of being transferred to trucks for a ferry trip. Some wonder whether Britain will be ready for it. A writer on the project said, \"They're still arguing about whether it will introduce rabies into this country, or let in terrorists\". Critics say obsolete commuter lines in SE England will cause delays.\n",
      "\n",
      "📄 Document ID: AP891017-0204\n",
      "\n",
      "<DOC>\n",
      "<DOCNO> AP891017-0204 </DOCNO>\n",
      "<FILEID>AP-NR-10-17-89 2325EDT</FILEID>\n",
      "<FIRST>r a AM-Quake-Odds     10-17 0456</FIRST>\n",
      "<SECOND>AM-Quake-Odds,0468</SECOND>\n",
      "<HEAD>Area Where Earthquake Hit Seen as Highly Probable in 1988 Report</HEAD>\n",
      "<HEAD>With AM-SF Quake</HEAD>\n",
      "<BYLINE>By MICHAEL FLEEMAN</BY...\n",
      "\n",
      "🟨 Matching Gold Summary:\n",
      "The earthquake in the Bay area on Tuesday had been targeted as likely to have one by the U.S. Geological Survey in 1998. The prediction was based on time elapse since the 1906 San Francisco quake, among other factors. The quake registered 6.9 on the Richter scale and was centered in the Santa Cruz, California, area. Reports indicated widespread damage, especially on highways and old masonry buildings. A quake of 7 on the Richter scale can cause major heavy damage. Some Southern California areas have an even higher probability of a major quake, the highest being Bakersfield in central California with a 90% probability.\n"
     ]
    }
   ],
   "source": [
    "duc_path = \"DUC2001\"\n",
    "documents, gold_summaries = load_documents(duc_path)\n",
    "\n",
    "for i, (doc_id, text) in enumerate(documents.items()):\n",
    "    print(f\"\\n📄 Document ID: {doc_id}\\n{text[:300]}...\")\n",
    "    doc_id = doc_id.lower()\n",
    "    if doc_id in gold_summaries:\n",
    "        print(f\"\\n🟨 Matching Gold Summary:\\n{gold_summaries[doc_id]}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No matching summary found.\")\n",
    "    \n",
    "    if i >= 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        filtered_words = [stemmer.stem(w) for w in words if w.isalnum() and w not in stop_words]\n",
    "        preprocessed_sentences.append(filtered_words)\n",
    "    \n",
    "    return sentences, preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹Original sentence example:\n",
      "<DOC>\n",
      "<DOCNO> FBIS3-41815 </DOCNO>\n",
      "<HT>    \"jptep001__l94053\" </HT>\n",
      "\n",
      "\n",
      "<HEADER>\n",
      "<AU>   JPRS-TEP-94-001-L </AU>\n",
      "Document Type:JPRS \n",
      "Document Title:Epidemiology \n",
      "\n",
      "<DATE1>  25 February 1994 </DATE1>\n",
      "\n",
      "</HEADER>\n",
      "\n",
      "<F P=100> WEST EUROPE </F>\n",
      "<F P=101> UNITED KINGDOM </F>\n",
      "<H3> <TI>   Concern Over Transmission of Spongiform Encephalopathy </TI></H3>\n",
      "<F P=102>  94WE0069A London THE TIMES in English 13 Oct 93 p 12 -- FOR \n",
      "OFFICIAL USE ONLY </F>\n",
      "\n",
      "<F P=103> 94WE0069A </F>\n",
      "<F P=104>  London THE TIMES </F>\n",
      "\n",
      "\n",
      "<TEXT>\n",
      "Language: <F P=105> English </F>\n",
      "Article Type:CSO \n",
      "\n",
      "<F P=106> [Article by Nigel Hawkes, Science Editor: \"Zoo Antelope </F>\n",
      "Catch Mad Cow Disease\"] \n",
      "  [Text] Scientists at London zoo have discovered that a \n",
      "strain of \"mad cow disease\" affecting a type of antelope can be \n",
      "transmitted much more easily than was thought.\n",
      "\n",
      "🔹Tokenized and preprocessed:\n",
      "['doc', 'docno', 'ht', 'header', 'au', 'document', 'type', 'jpr', 'document', 'titl', 'epidemiolog', 'date1', '25', 'februari', '1994', 'f', 'west', 'europ', 'f', 'unit', 'kingdom', 'h3', 'ti', 'concern', 'transmiss', 'spongiform', 'encephalopathi', 'f', '94we0069a', 'london', 'time', 'english', '13', 'oct', '93', 'p', '12', 'offici', 'use', 'f', '94we0069a', 'f', 'london', 'time', 'text', 'languag', 'f', 'english', 'articl', 'type', 'cso', 'f', 'articl', 'nigel', 'hawk', 'scienc', 'editor', 'zoo', 'antelop', 'catch', 'mad', 'cow', 'diseas', 'text', 'scientist', 'london', 'zoo', 'discov', 'strain', 'mad', 'cow', 'diseas', 'affect', 'type', 'antelop', 'transmit', 'much', 'easili', 'thought']\n"
     ]
    }
   ],
   "source": [
    "sample_doc_id = list(documents.keys())[0]\n",
    "sentences, tokenized = preprocess(documents[sample_doc_id])\n",
    "\n",
    "print(\"🔹Original sentence example:\")\n",
    "print(sentences[0])\n",
    "\n",
    "print(\"\\n🔹Tokenized and preprocessed:\")\n",
    "print(tokenized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution(word_lists):\n",
    "    counter = Counter()\n",
    "    for word_list in word_lists:\n",
    "        counter.update(word_list)\n",
    "    \n",
    "    total = sum(counter.values())\n",
    "    distribution = {word: count / total for word, count in counter.items()}\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_sum_word_based(original_sentences, tokenized_sentences, PD, max_sentences=5):\n",
    "    selected = []\n",
    "    used_indices = set()\n",
    "\n",
    "    while len(selected) < max_sentences:\n",
    "        min_kl = float('inf')\n",
    "        best_idx = -1\n",
    "        for i, sentence_tokens in enumerate(tokenized_sentences):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            summary_tokens = []\n",
    "            for j in selected:\n",
    "                summary_tokens.extend(tokenized_sentences[j])\n",
    "            summary_tokens.extend(sentence_tokens)\n",
    "            \n",
    "            PS = compute_distribution([summary_tokens])\n",
    "            \n",
    "            all_words = set(PD.keys()).union(set(PS.keys()))\n",
    "            pd_vec = np.array([PD.get(w, 1e-10) for w in all_words])\n",
    "            ps_vec = np.array([PS.get(w, 1e-10) for w in all_words])\n",
    "            \n",
    "            kl_div = entropy(pd_vec, ps_vec)\n",
    "            \n",
    "            if kl_div < min_kl:\n",
    "                min_kl = kl_div\n",
    "                best_idx = i\n",
    "        \n",
    "        if best_idx != -1:\n",
    "            used_indices.add(best_idx)\n",
    "            selected.append(best_idx)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ' '.join([original_sentences[i] for i in selected])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document: fbis-41815\n",
      "\n",
      "---Generated Summary---\n",
      "<DOC>\n",
      "<DOCNO> FBIS3-41815 </DOCNO>\n",
      "<HT>    \"jptep001__l94053\" </HT>\n",
      "\n",
      "\n",
      "<HEADER>\n",
      "<AU>   JPRS-TEP-94-001-L </AU>\n",
      "Document Type:JPRS \n",
      "Document Title:Epidemiology \n",
      "\n",
      "<DATE1>  25 February 1994 </DATE1>\n",
      "\n",
      "</HEADER>\n",
      "\n",
      "<F P=100> WEST EUROPE </F>\n",
      "<F P=101> UNITED KINGDOM </F>\n",
      "<H3> <TI>   Concern Over Transmission of Spongiform Encephalopathy </TI></H3>\n",
      "<F P=102>  94WE0069A London THE TIMES in English 13 Oct 93 p 12 -- FOR \n",
      "OFFICIAL USE ONLY </F>\n",
      "\n",
      "<F P=103> 94WE0069A </F>\n",
      "<F P=104>  London THE TIMES </F>\n",
      "\n",
      "\n",
      "<TEXT>\n",
      "Language: <F P=105> English </F>\n",
      "Article Type:CSO \n",
      "\n",
      "<F P=106> [Article by Nigel Hawkes, Science Editor: \"Zoo Antelope </F>\n",
      "Catch Mad Cow Disease\"] \n",
      "  [Text] Scientists at London zoo have discovered that a \n",
      "strain of \"mad cow disease\" affecting a type of antelope can be \n",
      "transmitted much more easily than was thought. Another danger taken seriously by the zoo, a world centre \n",
      "for \n",
      "breeding rare and endangered species, is that animals bred in \n",
      "captivity could carry the infection when released into the wild. Sheep are believed to catch \n",
      "the disease by contact with placentas in fields after births, \n",
      "but in the case of the kudu even this route seems unlikely. It is more likely, the scientists believe, that \n",
      "an unidentified agent entered the herd in contaminated feed and \n",
      "was passed along, as with more mundane infections. It has also been found in domestic \n",
      "cats and their larger relations, the cheetah and the puma, in \n",
      "eland and nyala, and in the gemsbok and the Arabian oryx.\n",
      "\n",
      "---Gold Summary (Abstract)---\n",
      "\n",
      "\n",
      "Document: ap881017-0235\n",
      "\n",
      "---Generated Summary---\n",
      "\n",
      "<DOC>\n",
      "<DOCNO> AP881017-0235 </DOCNO>\n",
      "<FILEID>AP-NR-10-17-88 2351EDT</FILEID>\n",
      "<FIRST>a i BC-ChannelTunnel ADV30   10-17 0995</FIRST>\n",
      "<SECOND>BC-Channel Tunnel, ADV 30,1025</SECOND>\n",
      "<NOTE>$Adv30</NOTE>\n",
      "<NOTE>For Release Sunday, Oct. 30, or Thereafter</NOTE>\n",
      "<HEAD>Gigantic Tunnel Project Inches Toward Joining England and France</HEAD>\n",
      "<BYLINE>By MARCUS ELIASON</BYLINE>\n",
      "<BYLINE>Associated Press Writer</BYLINE>\n",
      "<DATELINE>SHAKESPEARE CLIFF, England (AP) </DATELINE>\n",
      "<TEXT>\n",
      "   A colossal tunneling machine\n",
      "is boring beneath the English Channel from the white cliffs of\n",
      "Dover, pursuing a dream born in Napoleon's time that is coming true\n",
      "at last. Eurotunnel will run shuttle trains once every\n",
      "three minutes at peak times between terminals near Folkestone and\n",
      "Calais, and British Rail and the Frenh state railroad will operate\n",
      "trains from London and Paris. Britain's entry into the European Economic Community engendered\n",
      "a spirit of unity and the digging began again in 1974, but two\n",
      "years later a new British government shelved the project. Tunneling speed at the Dover end is less than 15 feet an hour\n",
      "and the machine boring from the geologically more complex French\n",
      "end moves even slower, which is why the tunnel will not open until\n",
      "1993. With trade barriers among the 12 EEC countries set to fall in\n",
      "1992, Colin Kirkland, technical director of Eurotunnel's on the\n",
      "British side, says the tunnel will be completed this time.\n",
      "\n",
      "---Gold Summary (Abstract)---\n",
      "Construction of a 31-mile tunnel, 24 miles of it underwater, is underway between Dover, England to the coast of France. Scheduled to be completed in 1993, it will cut the London-Paris journey from six hours to three. It will enable freight to travel on a single train instead of being transferred to trucks for a ferry trip. Some wonder whether Britain will be ready for it. A writer on the project said, \"They're still arguing about whether it will introduce rabies into this country, or let in terrorists\". Critics say obsolete commuter lines in SE England will cause delays.\n",
      "\n",
      "Document: ap891017-0204\n",
      "\n",
      "---Generated Summary---\n",
      "</TEXT>\n",
      "</DOC> \n",
      "<DOC>\n",
      "<DOCNO> AP891017-0204 </DOCNO>\n",
      "<FILEID>AP-NR-10-17-89 2325EDT</FILEID>\n",
      "<FIRST>r a AM-Quake-Odds     10-17 0456</FIRST>\n",
      "<SECOND>AM-Quake-Odds,0468</SECOND>\n",
      "<HEAD>Area Where Earthquake Hit Seen as Highly Probable in 1988 Report</HEAD>\n",
      "<HEAD>With AM-SF Quake</HEAD>\n",
      "<BYLINE>By MICHAEL FLEEMAN</BYLINE>\n",
      "<BYLINE>Associated Press Writer</BYLINE>\n",
      "<DATELINE>PASADENA, Calif. (AP) </DATELINE>\n",
      "<TEXT>\n",
      "   The major earthquake that struck the San\n",
      "Francisco Bay area Tuesday occurred in a region seismologists\n",
      "targeted as having the highest probability of a strong quake in\n",
      "Northern California. The damage in the Bay area occurred to the same kind of\n",
      "structures heavily damaged in the 1987 Whittier quake in the Los\n",
      "Angeles area, which registered a 5.9 Richter reading, Allen said. A 1988 report by the U.S. Geological Survey placed the\n",
      "probability of an earthquake of 6.5 magnitude on the Richter scale\n",
      "at 30 percent by the year 2018 in the Southern Santa Cruz mountains. The high probability is based on several factors, including\n",
      "length of time since the last major earthquake struck the area in\n",
      "1906, said Clarence Allen, professor of Geology and Geophysics at\n",
      "the California Institue of Technology in Pasadena.\n",
      "\n",
      "---Gold Summary (Abstract)---\n",
      "The earthquake in the Bay area on Tuesday had been targeted as likely to have one by the U.S. Geological Survey in 1998. The prediction was based on time elapse since the 1906 San Francisco quake, among other factors. The quake registered 6.9 on the Richter scale and was centered in the Santa Cruz, California, area. Reports indicated widespread damage, especially on highways and old masonry buildings. A quake of 7 on the Richter scale can cause major heavy damage. Some Southern California areas have an even higher probability of a major quake, the highest being Bakersfield in central California with a 90% probability.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for doc_id, doc_text in list(documents.items())[:3]:  \n",
    "    doc_id = doc_id.lower()\n",
    "    sentences, tokenized = preprocess(doc_text)\n",
    "    PD = compute_distribution(tokenized)\n",
    "    generated_summary = kl_sum_word_based(sentences, tokenized, PD, max_sentences=5)\n",
    "\n",
    "    print(f\"\\nDocument: {doc_id}\")\n",
    "    print(\"\\n---Generated Summary---\")\n",
    "    print(generated_summary)\n",
    "    \n",
    "    if doc_id in gold_summaries:\n",
    "        print(\"\\n---Gold Summary (Abstract)---\")\n",
    "        print(gold_summaries[doc_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---ROUGE Scores---\n",
      "{'rouge1': Score(precision=0.3399014778325123, recall=0.6509433962264151, fmeasure=0.4466019417475728), 'rouge2': Score(precision=0.09900990099009901, recall=0.19047619047619047, fmeasure=0.13029315960912055), 'rougeL': Score(precision=0.1625615763546798, recall=0.3113207547169811, fmeasure=0.21359223300970875)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(gold_summaries[doc_id], generated_summary)\n",
    "print(\"\\n---ROUGE Scores---\")\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
