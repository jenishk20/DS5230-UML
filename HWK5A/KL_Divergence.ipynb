{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pyrouge\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.stats import entropy  # KL Divergence\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ------------------------ Preprocessing Functions ------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "def get_word_distribution(text):\n",
    "    words = preprocess_text(text)\n",
    "    word_counts = Counter(words)\n",
    "    total_words = sum(word_counts.values())\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "# ------------------------ KL-Sum (Word-Based) ------------------------\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\" Compute KL divergence between distributions p and q (avoiding log(0)). \"\"\"\n",
    "    p = np.array(list(p.values()))\n",
    "    q = np.array(list(q.values()))\n",
    "\n",
    "    # Smoothing: Add small value to avoid log(0)\n",
    "    q = np.where(q == 0, 1e-10, q)\n",
    "    return entropy(p, q)\n",
    "\n",
    "def kl_sum_word_based(document, summary_length):\n",
    "    sentences = sent_tokenize(document)\n",
    "    PD = get_word_distribution(document)  # Fixed document distribution\n",
    "    summary = []\n",
    "    PS = Counter()\n",
    "\n",
    "    while len(summary) < summary_length and len(sentences) > 0:\n",
    "        min_kl_sentence = None\n",
    "        min_kl_value = float('inf')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            temp_PS = PS.copy()\n",
    "            temp_PS.update(preprocess_text(sentence))\n",
    "            temp_total = sum(temp_PS.values())\n",
    "            temp_PS = {word: count / temp_total for word, count in temp_PS.items()}\n",
    "\n",
    "            kl_value = kl_divergence(temp_PS, PD)\n",
    "            if kl_value < min_kl_value:\n",
    "                min_kl_value = kl_value\n",
    "                min_kl_sentence = sentence\n",
    "\n",
    "        if min_kl_sentence:\n",
    "            summary.append(min_kl_sentence)\n",
    "            PS.update(preprocess_text(min_kl_sentence))\n",
    "            sentences.remove(min_kl_sentence)\n",
    "\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# ------------------------ KL-Sum (Topic-Based with LDA) ------------------------\n",
    "\n",
    "def get_lda_topic_distribution(text, lda_model, vectorizer):\n",
    "    \"\"\" Compute topic distribution for a given text using LDA model. \"\"\"\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    return lda_model.transform(text_vectorized)[0]  # Probability distribution\n",
    "\n",
    "def kl_sum_topic_based(document, summary_length, lda_model, vectorizer):\n",
    "    sentences = sent_tokenize(document)\n",
    "    PD = get_lda_topic_distribution(document, lda_model, vectorizer)\n",
    "    summary = []\n",
    "    PS = np.zeros(len(PD))  # Initialize uniform topic distribution\n",
    "\n",
    "    while len(summary) < summary_length and len(sentences) > 0:\n",
    "        min_kl_sentence = None\n",
    "        min_kl_value = float('inf')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            temp_PS = PS.copy()\n",
    "            temp_PS += get_lda_topic_distribution(sentence, lda_model, vectorizer)\n",
    "            temp_PS /= temp_PS.sum()  # Normalize\n",
    "\n",
    "            kl_value = entropy(temp_PS, PD)\n",
    "            if kl_value < min_kl_value:\n",
    "                min_kl_value = kl_value\n",
    "                min_kl_sentence = sentence\n",
    "\n",
    "        if min_kl_sentence:\n",
    "            summary.append(min_kl_sentence)\n",
    "            PS += get_lda_topic_distribution(min_kl_sentence, lda_model, vectorizer)\n",
    "            PS /= PS.sum()\n",
    "            sentences.remove(min_kl_sentence)\n",
    "\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# ------------------------ ROUGE Evaluation ------------------------\n",
    "\n",
    "def evaluate_with_rouge(pred_summaries, gold_summaries_dir):\n",
    "    \"\"\" Evaluate generated summaries against gold-standard summaries using ROUGE. \"\"\"\n",
    "    rouge = pyrouge.Rouge155()\n",
    "    rouge.system_dir = \"generated_summaries/\"\n",
    "    rouge.model_dir = gold_summaries_dir\n",
    "    rouge.system_filename_pattern = 'summary.(\\d+).txt'\n",
    "    rouge.model_filename_pattern = 'gold_summary.#ID#.txt'\n",
    "    \n",
    "    rouge_results = rouge.convert_and_evaluate()\n",
    "    print(rouge_results)\n",
    "\n",
    "# ------------------------ Running on Datasets ------------------------\n",
    "\n",
    "# Load DUC Dataset\n",
    "duc_data_dir = \"DUC_Dataset/Documents/\"\n",
    "duc_gold_dir = \"DUC_Dataset/Summaries/\"\n",
    "\n",
    "for file in os.listdir(duc_data_dir):\n",
    "    with open(os.path.join(duc_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "\n",
    "    # KL-Sum (Word-Based)\n",
    "    summary_word = kl_sum_word_based(document, summary_length=5)\n",
    "    with open(f\"generated_summaries/summary.{file}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_word)\n",
    "\n",
    "    # KL-Sum (Topic-Based)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform([document])\n",
    "    lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    summary_topic = kl_sum_topic_based(document, summary_length=5, lda_model=lda, vectorizer=vectorizer)\n",
    "    with open(f\"generated_summaries/summary_topic.{file}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_topic)\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "evaluate_with_rouge(\"generated_summaries/\", duc_gold_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
