{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jenishkothari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jenishkothari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from scipy.stats import entropy\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(duc_path):\n",
    "    docs_path = os.path.join(duc_path, \"docs\")\n",
    "    summaries_path = os.path.join(duc_path, \"Summaries\")\n",
    "    \n",
    "    documents = {}\n",
    "    gold_summaries = {}\n",
    "\n",
    "    for filename in os.listdir(docs_path):\n",
    "        filepath = os.path.join(docs_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                documents[filename] = f.read()\n",
    "    \n",
    "    for filename in os.listdir(summaries_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(summaries_path, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                match = re.search(r'Abstract:(.*?)(?:Introduction:|$)', content, re.DOTALL)\n",
    "                if match:\n",
    "                    doc_key = filename.replace(\".txt\", \"\")\n",
    "                    gold_summaries[doc_key] = match.group(1).strip()\n",
    "    \n",
    "    return documents, gold_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        filtered_words = [stemmer.stem(w) for w in words if w.isalnum() and w not in stop_words]\n",
    "        preprocessed_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    return sentences, preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(documents, n_topics=30):\n",
    "    all_cleaned_docs = []\n",
    "\n",
    "    for doc in documents.values():\n",
    "        _, cleaned = preprocess(doc)\n",
    "        all_cleaned_docs.append(' '.join(cleaned))  # full doc as one string\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    doc_term_matrix = vectorizer.fit_transform(all_cleaned_docs)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\n",
    "    lda_model.fit(doc_term_matrix)\n",
    "    \n",
    "    return lda_model, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(text_list, vectorizer, lda_model, n_topics):\n",
    "    text = ' '.join(text_list)\n",
    "    X = vectorizer.transform([text])\n",
    "    topic_distribution = lda_model.transform(X)[0]\n",
    "    topic_distribution = np.where(topic_distribution == 0, 1e-10, topic_distribution)\n",
    "    return topic_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_sum_topic_based(original_sentences, cleaned_sentences, vectorizer, lda_model, PD, n_topics=30, max_sentences=5):\n",
    "    selected = []\n",
    "    used_indices = set()\n",
    "\n",
    "    while len(selected) < max_sentences:\n",
    "        min_kl = float('inf')\n",
    "        best_idx = -1\n",
    "\n",
    "        for i, cleaned_sentence in enumerate(cleaned_sentences):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "\n",
    "            # Build candidate summary\n",
    "            summary_text = [cleaned_sentences[j] for j in selected] + [cleaned_sentence]\n",
    "            PS = get_topic_distribution(summary_text, vectorizer, lda_model, n_topics)\n",
    "            kl_div = entropy(PD, PS)\n",
    "\n",
    "            if kl_div < min_kl:\n",
    "                min_kl = kl_div\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx != -1:\n",
    "            used_indices.add(best_idx)\n",
    "            selected.append(best_idx)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ' '.join([original_sentences[i] for i in selected])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ƒ Generated Summary:\n",
      "\n",
      "<DOC>\n",
      "<DOCNO> AP900629-0260 </DOCNO>\n",
      "<FILEID>AP-NR-06-29-90 0653EDT</FILEID>\n",
      "<FIRST>a f BC-Chunnel Adv02   06-29 1083</FIRST>\n",
      "<SECOND>BC-Chunnel, Adv 02,1122</SECOND>\n",
      "<NOTE>$adv02</NOTE>\n",
      "<NOTE>For release Monday July 2</NOTE>\n",
      "<HEAD>England-France Tunnel Halfway There Despite Problems</HEAD>\n",
      "<HEAD>LaserPhoto LON23 of June 26; Graphic</HEAD>\n",
      "<BYLINE>By COTTEN TIMBERLAKE</BYLINE>\n",
      "<BYLINE>Associated Press Writer</BYLINE>\n",
      "<DATELINE>LONDON (AP) </DATELINE>\n",
      "<TEXT>\n",
      "   It's been described as the largest current civil\n",
      "engineering project, a multibillion dollar link that will help\n",
      "revolutionize Europe's economy and physically end Britain's\n",
      "historic isolation, a dream born in Napoleon's day. In October, concern about the rising pricetag drove Eurotunnel's\n",
      "banking syndicate to freeze funds for three months until the\n",
      "company reached a truce with Trans-Manche Link, the consortium of\n",
      "10 British and French contractors doing the construction, over\n",
      "responsibility for $1.7 billion in overruns. The Chunnel's June 15, 1993 scheduled debut will come six months\n",
      "after the 12-nation European Community formally drops remaining\n",
      "trade barriers and becomes a unified marketplace of 320 million\n",
      "consumers. ``Britain becomes branch line of Europe,'' a Guardian newspaper\n",
      "headline declared after the government announced June 14 that it\n",
      "would not fund a high-speed rail link between London and the\n",
      "British end of the tunnel. Giant boring machines are digging three tunnels toward each\n",
      "other from Folkestone, England and Calais, France, with the first\n",
      "underground meeting expected in November in the service tunnel\n",
      "between the rail tunnels.\n",
      "\n",
      "ðŸŸ¨ Gold Summary:\n",
      "The \"Chunnel\" between Britain and France is half dug, scheduled to open in three years on time, but is almost 60% over budget, embroiled in contractor dispute over responsibility for $1.7 billion in overruns, and scrambling for more investment money after the British government refused to finance a rail link. The British public shows little enthusiasm for the Chunnel, fearing it will import rabies, terrorists and invading armies. The French are forging ahead with a high-speed rail link, enthusiastic because the tunnel surfaces in one of their more depressed areas. Rural southeast English are reluctant to have high-speed trains screaming through their back yards.\n"
     ]
    }
   ],
   "source": [
    "duc_path = \"DUC2001\"\n",
    "documents, gold_summaries = load_documents(duc_path)\n",
    "lda_model, vectorizer = train_lda(documents, n_topics=30)\n",
    "\n",
    "# Pick one doc\n",
    "doc_id = list(documents.keys())[3]\n",
    "sentences, cleaned = preprocess(documents[doc_id])\n",
    "PD = get_topic_distribution(cleaned, vectorizer, lda_model, n_topics=30)\n",
    "\n",
    "# Generate summary\n",
    "generated_summary = kl_sum_topic_based(sentences, cleaned, vectorizer, lda_model, PD, n_topics=30, max_sentences=5)\n",
    "\n",
    "print(\"ðŸ“ƒ Generated Summary:\")\n",
    "print(generated_summary)\n",
    "doc_id = doc_id.lower()\n",
    "if doc_id in gold_summaries:\n",
    "    print(\"\\nðŸŸ¨ Gold Summary:\")\n",
    "    print(gold_summaries[doc_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
