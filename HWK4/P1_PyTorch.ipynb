{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # should be >= 1.13.1\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "get_data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Northeastern/DS5230_backup/myenv/lib/python3.13/site-packages/sklearn/utils/_bunch.py:57\u001b[39m, in \u001b[36mBunch.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Northeastern/DS5230_backup/myenv/lib/python3.13/site-packages/sklearn/utils/_bunch.py:42\u001b[39m, in \u001b[36mBunch.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     38\u001b[39m     warnings.warn(\n\u001b[32m     39\u001b[39m         \u001b[38;5;28mself\u001b[39m._deprecated_key_to_warnings[key],\n\u001b[32m     40\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m     41\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'get_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --------------------------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# üì• Load MNIST from OpenML (ID = 554)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# --------------------------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m mnist = fetch_openml(name=\u001b[33m\"\u001b[39m\u001b[33mmnist_784\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X, y, _, _ = \u001b[43mmnist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m(target=mnist.default_target_attribute, dataset_format=\u001b[33m'\u001b[39m\u001b[33marray\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[32m     22\u001b[39m X = X.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m).astype(np.float32) / \u001b[32m255.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Northeastern/DS5230_backup/myenv/lib/python3.13/site-packages/sklearn/utils/_bunch.py:59\u001b[39m, in \u001b[36mBunch.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(key)\n",
      "\u001b[31mAttributeError\u001b[39m: get_data"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------\n",
    "# ‚öôÔ∏è Device Setup\n",
    "# --------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üì• Load MNIST from OpenML (ID = 554)\n",
    "# --------------------------------------------\n",
    "mnist = fetch_openml(name=\"mnist_784\", version=1)\n",
    "X, y, _, _ = mnist.get_data(target=mnist.default_target_attribute, dataset_format='array')\n",
    "\n",
    "# Preprocessing\n",
    "X = X.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "# Dataset\n",
    "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# 80/20 split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß† CNN Model\n",
    "# --------------------------------------------\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß™ Training & Evaluation Functions\n",
    "# --------------------------------------------\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üöÄ Run Training\n",
    "# --------------------------------------------\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ‚úÖ Final Accuracy Report\n",
    "# --------------------------------------------\n",
    "print(\"\\nEvaluating on 20% test split from OpenML...\")\n",
    "final_loss, final_acc = evaluate(model, val_loader, criterion)\n",
    "print(f\"Final Validation Accuracy: {final_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
