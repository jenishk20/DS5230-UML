{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "10783\n",
      "TextClassifier(\n",
      "  (embedding_layer): Embedding(19717, 100)\n",
      "  (fc): Linear(in_features=100, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n",
      "Epoch 1/40, Loss: 2.960785389829565\n",
      "Epoch 2/40, Loss: 2.8990357866993657\n",
      "Epoch 3/40, Loss: 2.8308806834397493\n",
      "Epoch 4/40, Loss: 2.747389549679226\n",
      "Epoch 5/40, Loss: 2.640270300264712\n",
      "Epoch 6/40, Loss: 2.5472207634537307\n",
      "Epoch 7/40, Loss: 2.453374895343074\n",
      "Epoch 8/40, Loss: 2.3917201545503404\n",
      "Epoch 9/40, Loss: 2.3275935835308497\n",
      "Epoch 10/40, Loss: 2.270705758642267\n",
      "Epoch 11/40, Loss: 2.2258672091696\n",
      "Epoch 12/40, Loss: 2.1893404139412773\n",
      "Epoch 13/40, Loss: 2.1480565631831134\n",
      "Epoch 14/40, Loss: 2.115941608393634\n",
      "Epoch 15/40, Loss: 2.08919463996534\n",
      "Epoch 16/40, Loss: 2.062292710940043\n",
      "Epoch 17/40, Loss: 2.030610822306739\n",
      "Epoch 18/40, Loss: 2.0074416783120896\n",
      "Epoch 19/40, Loss: 1.986762222537288\n",
      "Epoch 20/40, Loss: 1.9618773504539773\n",
      "Epoch 21/40, Loss: 1.9424958101025334\n",
      "Epoch 22/40, Loss: 1.9250947969931143\n",
      "Epoch 23/40, Loss: 1.8984982742203607\n",
      "Epoch 24/40, Loss: 1.8831295556492276\n",
      "Epoch 25/40, Loss: 1.8739187607058772\n",
      "Epoch 26/40, Loss: 1.8508762770228915\n",
      "Epoch 27/40, Loss: 1.827475087730973\n",
      "Epoch 28/40, Loss: 1.8226096175335071\n",
      "Epoch 29/40, Loss: 1.8009100251727634\n",
      "Epoch 30/40, Loss: 1.7875326448016697\n",
      "Epoch 31/40, Loss: 1.7803660207324559\n",
      "Epoch 32/40, Loss: 1.7608821988105774\n",
      "Epoch 33/40, Loss: 1.7552978877668028\n",
      "Epoch 34/40, Loss: 1.740994856092665\n",
      "Epoch 35/40, Loss: 1.7307657294803196\n",
      "Epoch 36/40, Loss: 1.7150096186885126\n",
      "Epoch 37/40, Loss: 1.7111932339491667\n",
      "Epoch 38/40, Loss: 1.6982139737517745\n",
      "Epoch 39/40, Loss: 1.6878247238971569\n",
      "Epoch 40/40, Loss: 1.6777503256444577\n",
      "Validation Accuracy: 44.22809457579972%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "freezzer = False\n",
    "nltk.download('punkt')\n",
    "\n",
    "categories = ['rec.sport.hockey', 'comp.graphics', 'sci.med', 'talk.politics.guns', 'soc.religion.christian']\n",
    "ng_data = fetch_20newsgroups(subset='train')\n",
    "min_size = 5 \n",
    "max_size = 1000\n",
    "tokens = []\n",
    "temp = []\n",
    "print(len(ng_data.data))\n",
    "for i in range(len(ng_data.data)):\n",
    "    a = word_tokenize(ng_data.data[i].lower())\n",
    "    if len(a) > min_size and len(a)<max_size:\n",
    "        tokens.append(a)\n",
    "        temp.append(ng_data.target[i])\n",
    "        \n",
    "print(len(tokens))\n",
    "\n",
    "min_frequency = 5\n",
    "\n",
    "word_counts = Counter([word for doc in tokens for word in doc])\n",
    "\n",
    "\n",
    "filtered_words = [word for word, count in word_counts.items() if count >= min_frequency]\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    glove_dict = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_dict[word] = vector\n",
    "    return glove_dict\n",
    "\n",
    "glove_dict = load_glove_embeddings(\"glove.6B.100d.txt\")\n",
    "\n",
    "word_to_index = {}\n",
    "index = 0\n",
    "\n",
    "for word in filtered_words:\n",
    "    if word in glove_dict:\n",
    "        word_to_index[word] = index\n",
    "        index += 1\n",
    "word_to_index['<UNK>'] = len(word_to_index)\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "\n",
    "def doc2ind(tokens, word_to_index):\n",
    "    return [word_to_index.get(word, word_to_index['<UNK>']) for word in tokens]\n",
    "\n",
    "\n",
    "\n",
    "ng_vector_idx = [doc2ind(doc, word_to_index) for doc in tokens]\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    return pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=padding_value)\n",
    "\n",
    "\n",
    "ng_padded = pad_sequences(ng_vector_idx)\n",
    "\n",
    "# Encode labels (target categories)\n",
    "label_encoder = LabelEncoder()\n",
    "ng_labels = label_encoder.fit_transform(temp)\n",
    "\n",
    "# Split \n",
    "X_train, X_val, y_train, y_val = train_test_split(ng_padded, ng_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network \n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, num_classes, glove_dict, freezzer):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        \n",
    "        for word, idx in word_to_index.items():\n",
    "            if word in glove_dict:\n",
    "                embedding_matrix[idx] = glove_dict[word]  \n",
    "        \n",
    "        self.embedding_layer.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "        self.embedding_layer.weight.requires_grad = freezzer  # Freeze the GloVe embeddings\n",
    "\n",
    "        # Define a fully connected layer for classification\n",
    "        self.fc = nn.Linear(embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)  \n",
    "        x = x.mean(dim=1)  \n",
    "        x = self.fc(x)\n",
    "        x = torch.relu(x) \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = len(set(ng_labels))\n",
    "model = TextClassifier(embedding_dim=100, vocab_size=vocab_size, num_classes=num_classes, glove_dict=glove_dict, freezzer = False)\n",
    "\n",
    "print(model)\n",
    "\n",
    "class NewsgroupDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = NewsgroupDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = NewsgroupDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding_layer): Embedding(19717, 100)\n",
      "  (fc): Linear(in_features=100, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n",
      "Epoch 1/40, Loss: 2.9507897271050347\n",
      "Epoch 2/40, Loss: 2.790778586599562\n",
      "Epoch 3/40, Loss: 2.4784791451913337\n",
      "Epoch 4/40, Loss: 2.1663900909600433\n",
      "Epoch 5/40, Loss: 1.91250788503223\n",
      "Epoch 6/40, Loss: 1.7049969200734738\n",
      "Epoch 7/40, Loss: 1.523323197276504\n",
      "Epoch 8/40, Loss: 1.3679495286058496\n",
      "Epoch 9/40, Loss: 1.241154784405673\n",
      "Epoch 10/40, Loss: 1.1174088992454387\n",
      "Epoch 11/40, Loss: 1.009323047487824\n",
      "Epoch 12/40, Loss: 0.9145931952529484\n",
      "Epoch 13/40, Loss: 0.8213542667803941\n",
      "Epoch 14/40, Loss: 0.732907067294474\n",
      "Epoch 15/40, Loss: 0.6696249450798388\n",
      "Epoch 16/40, Loss: 0.5875544992861924\n",
      "Epoch 17/40, Loss: 0.52528087121469\n",
      "Epoch 18/40, Loss: 0.4707951945839105\n",
      "Epoch 19/40, Loss: 0.42061749200026194\n",
      "Epoch 20/40, Loss: 0.37564025118395133\n",
      "Epoch 21/40, Loss: 0.33956517877954023\n",
      "Epoch 22/40, Loss: 0.2996701166309692\n",
      "Epoch 23/40, Loss: 0.2782805968489912\n",
      "Epoch 24/40, Loss: 0.23545615924177346\n",
      "Epoch 25/40, Loss: 0.22051635093435093\n",
      "Epoch 26/40, Loss: 0.20069175221853786\n",
      "Epoch 27/40, Loss: 0.17437392522891362\n",
      "Epoch 28/40, Loss: 0.15830442532896996\n",
      "Epoch 29/40, Loss: 0.14165879432801848\n",
      "Epoch 30/40, Loss: 0.12837467276387746\n",
      "Epoch 31/40, Loss: 0.11498087480388305\n",
      "Epoch 32/40, Loss: 0.10509856036277833\n",
      "Epoch 33/40, Loss: 0.09637138533095518\n",
      "Epoch 34/40, Loss: 0.08491781145265256\n",
      "Epoch 35/40, Loss: 0.07865230343821976\n",
      "Epoch 36/40, Loss: 0.0734557026376327\n",
      "Epoch 37/40, Loss: 0.061484325080420135\n",
      "Epoch 38/40, Loss: 0.05796793069414519\n",
      "Epoch 39/40, Loss: 0.05293136448081997\n",
      "Epoch 40/40, Loss: 0.0491078392981931\n",
      "Validation Accuracy: 80.89939731108021%\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(ng_labels)) \n",
    "model = TextClassifier(embedding_dim=100, vocab_size=vocab_size, num_classes=num_classes, glove_dict=glove_dict, freezzer=True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "class NewsgroupDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = NewsgroupDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = NewsgroupDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 40  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
